{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Base ChatBot From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-07 17:37:19--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txBlock\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2024-04-07 17:37:19 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how mant charachters we have:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"how mant charachters we have: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "## Inspect the first 500 characters\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Vocabulary Size\n",
    "### Get all the unique characters in text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "## first lets get number of characters by making a set\n",
    "numberofchars = sorted(list(set(text)))\n",
    "vocab_size = len(numberofchars)\n",
    "print(''.join(numberofchars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizung the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {c:i for i, c in enumerate(numberofchars)}\n",
    "dict2 = {i:c for i, c in enumerate(numberofchars)}\n",
    "\n",
    "# for encoding we take a string and return its tokenize interger version\n",
    "encoding = lambda Originalstring: [dict1[character] for character in Originalstring ]\n",
    "\n",
    "# for decoding we take a token sequence and return the characters\n",
    "decoding = lambda OriginalToken: ''.join([dict2 [token] for token in OriginalToken])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "print (encoding (\"hello world\"))\n",
    "print (decoding (encoding (\"hello world\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = torch.tensor(encoding(text),  dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "## Lets examine the data\n",
    "print(allData.shape)\n",
    "print(allData[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting The Data into Test and Training. For Transformers we typically only keep 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = allData[:int(0.9*len(allData))]\n",
    "testing_data  = allData[int(0.9*len(allData)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is :  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "y is :  tensor([47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "# this determines the size of the context for each prediction\n",
    "block = 8\n",
    "\n",
    "x = training_data [:block]\n",
    "y = training_data [1:block+1]\n",
    "\n",
    "print(\"x is : \", x)\n",
    "print (\"y is : \", y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we wants to see if we give an input of x_1, x_2, ..x_n what will be the expected output after each input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the input is tensor([18]) the expected output is 47\n",
      "when the input is tensor([18, 47]) the expected output is 56\n",
      "when the input is tensor([18, 47, 56]) the expected output is 57\n",
      "when the input is tensor([18, 47, 56, 57]) the expected output is 58\n",
      "when the input is tensor([18, 47, 56, 57, 58]) the expected output is 1\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1]) the expected output is 15\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15]) the expected output is 47\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the expected output is 58\n"
     ]
    }
   ],
   "source": [
    "for i in range(block):\n",
    "    inputs = x[:i+1]\n",
    "    outputs = y[i]\n",
    "    print(f\"when the input is {inputs} the expected output is {outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch maker funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([76, 50, 46, 57, 82])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(100-8,(5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what we are doing in the next function:\n",
    "### 1. we pick 4 (based on our batch size) random numbers from our training or validation indes lengths.  \n",
    "### 2. We will then define x as a stack of 4 batches each having 8 cols starting from first random number to random number + 8(size of block)\n",
    "### 3. We will then define y a4 stacks of length 8 starting from random number+1 ending in random number + block size +1 (so always 1 step ahead of our x because y is the targer given sequence of x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives us how many sequences of 8 length we wants to process simu\n",
    "batch= 4\n",
    "\n",
    "\n",
    "def batchMaker (splitType, batch, block):\n",
    "\n",
    "\n",
    "    if splitType == 'training':\n",
    "        data = training_data\n",
    "    else:\n",
    "        data = testing_data\n",
    "    randomcols = torch.randint(len(data) - block , (batch,))\n",
    "    x = torch.stack ([data[i:i+block] for i in randomcols])\n",
    "    y = torch.stack ([data[i+1:i+block+1]for i in randomcols])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our input is: \n",
      " tensor([[13, 52, 42,  1, 39, 50, 50,  1],\n",
      "        [25, 13, 30, 21, 13, 26, 13, 10],\n",
      "        [ 1, 51, 63,  1, 24, 53, 56, 42],\n",
      "        [39,  1, 46, 59, 52, 42, 56, 43]], device='cuda:0')\n",
      "our output is: \n",
      " tensor([[52, 42,  1, 39, 50, 50,  1, 58],\n",
      "        [13, 30, 21, 13, 26, 13, 10,  0],\n",
      "        [51, 63,  1, 24, 53, 56, 42,  1],\n",
      "        [ 1, 46, 59, 52, 42, 56, 43, 42]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "## testing our function\n",
    "\n",
    "x, y = batchMaker('training', batch, block)\n",
    "print (\"our input is: \\n\", x)\n",
    "print (\"our output is: \\n\", y )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the input sequence tensor([13], device='cuda:0'), our expected output is 52\n",
      "for the input sequence tensor([13, 52], device='cuda:0'), our expected output is 42\n",
      "for the input sequence tensor([13, 52, 42], device='cuda:0'), our expected output is 1\n",
      "for the input sequence tensor([13, 52, 42,  1], device='cuda:0'), our expected output is 39\n",
      "for the input sequence tensor([13, 52, 42,  1, 39], device='cuda:0'), our expected output is 50\n",
      "for the input sequence tensor([13, 52, 42,  1, 39, 50], device='cuda:0'), our expected output is 50\n",
      "for the input sequence tensor([13, 52, 42,  1, 39, 50, 50], device='cuda:0'), our expected output is 1\n",
      "for the input sequence tensor([13, 52, 42,  1, 39, 50, 50,  1], device='cuda:0'), our expected output is 58\n",
      "for the input sequence tensor([25], device='cuda:0'), our expected output is 13\n",
      "for the input sequence tensor([25, 13], device='cuda:0'), our expected output is 30\n",
      "for the input sequence tensor([25, 13, 30], device='cuda:0'), our expected output is 21\n",
      "for the input sequence tensor([25, 13, 30, 21], device='cuda:0'), our expected output is 13\n",
      "for the input sequence tensor([25, 13, 30, 21, 13], device='cuda:0'), our expected output is 26\n",
      "for the input sequence tensor([25, 13, 30, 21, 13, 26], device='cuda:0'), our expected output is 13\n",
      "for the input sequence tensor([25, 13, 30, 21, 13, 26, 13], device='cuda:0'), our expected output is 10\n",
      "for the input sequence tensor([25, 13, 30, 21, 13, 26, 13, 10], device='cuda:0'), our expected output is 0\n",
      "for the input sequence tensor([1], device='cuda:0'), our expected output is 51\n",
      "for the input sequence tensor([ 1, 51], device='cuda:0'), our expected output is 63\n",
      "for the input sequence tensor([ 1, 51, 63], device='cuda:0'), our expected output is 1\n",
      "for the input sequence tensor([ 1, 51, 63,  1], device='cuda:0'), our expected output is 24\n",
      "for the input sequence tensor([ 1, 51, 63,  1, 24], device='cuda:0'), our expected output is 53\n",
      "for the input sequence tensor([ 1, 51, 63,  1, 24, 53], device='cuda:0'), our expected output is 56\n",
      "for the input sequence tensor([ 1, 51, 63,  1, 24, 53, 56], device='cuda:0'), our expected output is 42\n",
      "for the input sequence tensor([ 1, 51, 63,  1, 24, 53, 56, 42], device='cuda:0'), our expected output is 1\n",
      "for the input sequence tensor([39], device='cuda:0'), our expected output is 1\n",
      "for the input sequence tensor([39,  1], device='cuda:0'), our expected output is 46\n",
      "for the input sequence tensor([39,  1, 46], device='cuda:0'), our expected output is 59\n",
      "for the input sequence tensor([39,  1, 46, 59], device='cuda:0'), our expected output is 52\n",
      "for the input sequence tensor([39,  1, 46, 59, 52], device='cuda:0'), our expected output is 42\n",
      "for the input sequence tensor([39,  1, 46, 59, 52, 42], device='cuda:0'), our expected output is 56\n",
      "for the input sequence tensor([39,  1, 46, 59, 52, 42, 56], device='cuda:0'), our expected output is 43\n",
      "for the input sequence tensor([39,  1, 46, 59, 52, 42, 56, 43], device='cuda:0'), our expected output is 42\n"
     ]
    }
   ],
   "source": [
    "for j in range(batch):\n",
    "    for i in range(block):\n",
    "        inputs = x[j,:i+1]\n",
    "        outputs = y[j,i]\n",
    "        print(f\"for the input sequence {inputs}, our expected output is {outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8637, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "H dxyEB:llgmozIk Kxl!;sg.jLtlidGxs$I:y-H$Gd!nRj\n",
      "IBxK'!-E&mvgtHzgb.B;ClOJ:xHKx?mLTuCS.?cbNb&k,Ov\n",
      "kiy-\n"
     ]
    }
   ],
   "source": [
    "class BigramModel (nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #make an embedding tabel size vocab size x vocab size (65 x 65) in this cqw3\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward (self, inputx, targets = None):\n",
    "\n",
    "        # table will be the token embedding of size (batch, block size , vocab size)\n",
    "        # (4 x 8 x 65)\n",
    "\n",
    "        predictions = self.token_embedding_table (inputx)\n",
    "\n",
    "        # if we dont have targets then no loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            BatchSize, BlockSize, VocabSize = predictions.shape\n",
    "\n",
    "            #predictions are in (batch x block x vocab size) dimention\n",
    "            # cross entropy wants (batch x vocab size x block )\n",
    "\n",
    "            predictions = predictions.view(BatchSize *BlockSize, VocabSize)\n",
    "            targets = targets.view(BatchSize *BlockSize)\n",
    "\n",
    "            # loss is the cross entropy of the predictions and targets\n",
    "\n",
    "            loss = F.cross_entropy(predictions, targets)\n",
    "\n",
    "        return predictions, loss\n",
    "\n",
    "    def generate(self, inputx, max):\n",
    "        # inputx is (Batch x vocan size) array\n",
    "        for _ in range(max):\n",
    "            # get the predictions\n",
    "            predictions, loss = self(inputx)\n",
    "            # focus only on the last time step\n",
    "            predictions = predictions[:, -1, :] # becomes (Batch, vocab size)\n",
    "            # get probabliries via softmax\n",
    "            probs = F.softmax(predictions, dim=-1) # (Batch, vocab size)\n",
    "\n",
    "            # sample from the distribution\n",
    "            indext_next = torch.multinomial(probs, num_samples=1) # (Batch, 1)\n",
    "\n",
    "            # concatinate the sample we got from above\n",
    "            inputx = torch.cat((inputx, indext_next), dim=1) # (Batch, block+1)\n",
    "        return inputx\n",
    "\n",
    "\n",
    "model = BigramModel(vocab_size).to(device)\n",
    "\n",
    "predictions, loss = model(x, y)\n",
    "print(predictions.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decoding(model.generate(inputx = torch.zeros((1, 1), dtype=torch.long, device=device), max=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.426051616668701\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range (100000):\n",
    "\n",
    "    #makes batches\n",
    "    x, y = batchMaker('training', batch, block)\n",
    "\n",
    "    #evaluate\n",
    "    predictions , loss = model (x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Th phes:\n",
      "\n",
      "BEN ascofo asu? s n mu kies pryouron iou er arouthe pred blle prethinol of ayse fithe cirathou whathat?-s\n",
      "Whelldacttlyo lovea hoooorvepeaney uthy thur an belavere t I od, ame. d wifes his me to candeense cchananden utorgidelit ms hichan fof liso!\n",
      "\n",
      "TE hafow\n",
      "S:\n",
      "NChary akeid harserou d\n",
      "BY:\n",
      "Cowag, s.\n",
      "ARI tcckfetenoy s wil.\n",
      "Isincathaconirero Corvarendlds s n cesir'\n",
      "\n",
      "To, tre muro vies r ot\n",
      "et wh, Andr uroindo jo and sh hed, dacert ins,\n",
      "Yodwommns I s woveasountatorathe?\n",
      "\n",
      "wed'sau hindeshlligin\n"
     ]
    }
   ],
   "source": [
    "print(decoding(model.generate(inputx = torch.zeros((1, 1), dtype=torch.long , device=device), max=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets get our model ready for GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of sequences we will process\n",
    "\n",
    "\n",
    "# maxcontent length for prediction\n",
    "batch = 32 # how many independent sequences will we process in parallel?\n",
    "block = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: training loss 4.5967, validation loss 4.6049\n",
      "step 300: training loss 2.7991, validation loss 2.8093\n",
      "step 600: training loss 2.5623, validation loss 2.5653\n",
      "step 900: training loss 2.5302, validation loss 2.5236\n",
      "step 1200: training loss 2.5069, validation loss 2.5008\n",
      "step 1500: training loss 2.4992, validation loss 2.5003\n",
      "step 1800: training loss 2.4987, validation loss 2.4887\n",
      "step 2100: training loss 2.5004, validation loss 2.4915\n",
      "step 2400: training loss 2.4983, validation loss 2.4905\n",
      "step 2700: training loss 2.4990, validation loss 2.4920\n",
      "step 3000: training loss 2.4929, validation loss 2.4759\n",
      "step 3300: training loss 2.4971, validation loss 2.4891\n",
      "step 3600: training loss 2.4972, validation loss 2.4966\n",
      "step 3900: training loss 2.4949, validation loss 2.4883\n",
      "step 4200: training loss 2.4965, validation loss 2.4921\n",
      "step 4500: training loss 2.4868, validation loss 2.4878\n",
      "step 4800: training loss 2.4938, validation loss 2.4862\n",
      "step 5100: training loss 2.4878, validation loss 2.4860\n",
      "step 5400: training loss 2.4882, validation loss 2.4935\n",
      "step 5700: training loss 2.4867, validation loss 2.4832\n",
      "step 6000: training loss 2.4931, validation loss 2.4938\n",
      "step 6300: training loss 2.4801, validation loss 2.4819\n",
      "step 6600: training loss 2.4895, validation loss 2.4802\n",
      "step 6900: training loss 2.4907, validation loss 2.4829\n",
      "step 7200: training loss 2.4861, validation loss 2.4817\n",
      "step 7500: training loss 2.4802, validation loss 2.4858\n",
      "step 7800: training loss 2.4849, validation loss 2.4857\n",
      "step 8100: training loss 2.4837, validation loss 2.4814\n",
      "step 8400: training loss 2.4875, validation loss 2.4818\n",
      "step 8700: training loss 2.4868, validation loss 2.4863\n",
      "step 9000: training loss 2.4901, validation loss 2.4840\n",
      "step 9300: training loss 2.4867, validation loss 2.4905\n",
      "step 9600: training loss 2.4780, validation loss 2.4801\n",
      "step 9900: training loss 2.4812, validation loss 2.4902\n",
      "\n",
      "An w burasal ggenis:\n",
      "Hepacr h se lithavou enkitc ghund weme yof the s nalyou.\n",
      "Seneyondyongos gon\n",
      "Y be ar maveasttit whore?\n",
      "Tha t milsur w irth wopart\n",
      "Dof nof y Al, sist'seeorouee thapreste k,\n",
      "HEY ind avioir't y my,\n",
      "sed.\n",
      "Wowinousse aneathene 'aken f he d ond l ashe feervemash t, ang thecost! bsthe cestellall.\n",
      "By th l Vofis thur sshe a d this bou Y: d dou,\n",
      "\n",
      "Yo het a rie;\n",
      "Athareay icooounond thee.\n",
      "Ite ot aby at'lod m geece stoshere sfilon hag pane;\n",
      "\n",
      "Ifop:\n",
      "INCIOK3 nd ad okilosth\n",
      "s ey Seet;\n",
      "V:\n",
      "Andica\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = batchMaker(split, batch, block)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "model = BigramModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: training loss {losses['train']:.4f}, validation loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    x, y = batchMaker('training', batch, block)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decoding(m.generate(context, max=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Changes to make the tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath= 64 # how many independent sequences will we process in parallel?\n",
    "block = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "numberOfEmbeddings = 384\n",
    "NumberofHeads = 6\n",
    "numberOfLayers = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the seld-attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(numberOfEmbeddings, head_size, bias=False)\n",
    "        self.query = nn.Linear(numberOfEmbeddings, head_size, bias=False)\n",
    "        self.value = nn.Linear(numberOfEmbeddings, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block, block)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, block, vocab size)\n",
    "        # output of size (batch, block, head size)\n",
    "        Batch,Block,Vocab = x.shape\n",
    "        k = self.key(x)   # (Batch,Block,head)\n",
    "        q = self.query(x) # (Batch,Batch,heas)\n",
    "\n",
    "        # attention scores\n",
    "        weight = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (Bath, Batch, hs) @ (Bath, hs, Batch) -> (Bath, batch_size, batch_size)\n",
    "        weight = weight.masked_fill(self.tril[:Block, :Block] == 0, float('-inf')) # (Bath, Block, Block)\n",
    "        weight = F.softmax(weight, dim=-1) # (Bath, Block, Block)\n",
    "        weight = self.dropout(weight)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (Bath,Block,hs)\n",
    "        out = weight @ v # (Bath, Block, Block) @ (Bath, Block, hs) -> (Bath, Block, hs)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention  \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([OneHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, numberOfEmbeddings)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" linear layerand a non-linear after \"\"\"\n",
    "\n",
    "    def __init__(self, numberOfEmbeddings):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(numberOfEmbeddings, 4 * numberOfEmbeddings),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * numberOfEmbeddings, numberOfEmbeddings),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, numberOfEmbeddings, NumberofHeads):\n",
    "        # numberOfEmbeddings: embedding dimension, NumberofHeads: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = numberOfEmbeddings // NumberofHeads\n",
    "        self.sa = MultiAttention(NumberofHeads, head_size)\n",
    "        self.ffwd = FeedFoward(numberOfEmbeddings)\n",
    "        self.ln1 = nn.LayerNorm(numberOfEmbeddings)\n",
    "        self.ln2 = nn.LayerNorm(numberOfEmbeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GhatBotModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, numberOfEmbeddings)\n",
    "        self.position_embedding_table = nn.Embedding(block, numberOfEmbeddings)\n",
    "        self.blocks = nn.Sequential(*[Block(numberOfEmbeddings, NumberofHeads) for _ in range(NumberofHeads)])\n",
    "        self.ln_f = nn.LayerNorm(numberOfEmbeddings) # final layer norm\n",
    "        self.lm_head = nn.Linear(numberOfEmbeddings, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        Batch, Blocks = idx.shape\n",
    "\n",
    "        # idx and targets are both (Batch,Blocks) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (Batch,Blocks,vocab_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(Blocks, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (Batch,Blocks,vocab_size)\n",
    "        x = self.blocks(x) # (Batch,Blocks,vocab_size)\n",
    "        x = self.ln_f(x) # (Batch,Blocks,vocab_size)\n",
    "        logits = self.lm_head(x) # (Batch,Blocks,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            Batch, Blocks, Vocabs = logits.shape\n",
    "            logits = logits.view(Batch*Blocks, Vocabs)\n",
    "            targets = targets.view(Batch*Blocks)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n",
      "step 0: training loss 4.1690, validation loss 4.1698\n",
      "step 500: training loss 2.0377, validation loss 2.0380\n",
      "step 1000: training loss 1.7176, validation loss 1.7210\n",
      "step 1500: training loss 1.6135, validation loss 1.6116\n",
      "step 2000: training loss 1.5469, validation loss 1.5446\n",
      "step 2500: training loss 1.5163, validation loss 1.5169\n",
      "step 3000: training loss 1.5069, validation loss 1.5055\n",
      "step 3500: training loss 1.4943, validation loss 1.4956\n",
      "step 4000: training loss 1.4829, validation loss 1.4806\n",
      "step 4500: training loss 1.4819, validation loss 1.4773\n",
      "step 5000: training loss 1.4869, validation loss 1.4905\n",
      "step 5500: training loss 1.4857, validation loss 1.4925\n",
      "step 6000: training loss 1.5092, validation loss 1.5056\n",
      "step 6500: training loss 1.5173, validation loss 1.5219\n",
      "step 7000: training loss 1.5345, validation loss 1.5232\n",
      "step 7500: training loss 1.5483, validation loss 1.5424\n",
      "step 8000: training loss 1.5534, validation loss 1.5574\n",
      "step 8500: training loss 1.5706, validation loss 1.5670\n",
      "step 9000: training loss 1.5822, validation loss 1.5862\n",
      "step 9500: training loss 1.6044, validation loss 1.6125\n",
      "step 9999: training loss 1.6359, validation loss 1.6346\n"
     ]
    }
   ],
   "source": [
    "model = GhatBotModel()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "#PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: training loss {losses['train']:.4f}, validation loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample batch\n",
    "    x, y = batchMaker('training', batch, block)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GLOUCESTER:\n",
      "And hand, as we are deare, too we to him:\n",
      "And, in dear some confess of nature's eye\n",
      "Where Oxford had beshew some soften to the noise;\n",
      "For yet would make it excellence again: the vant\n",
      "I have had charged, such a cient many to tean\n",
      "To flate peace in his place.\n",
      "\n",
      "GLOUCESTER:\n",
      "Part, look that we more harsh our breaths.\n",
      "Take up within cur are: therefore I thence came;\n",
      "Assist us, Couroy abound will he last.\n",
      "\n",
      "BONA:\n",
      "Indeed, I fear I have disness'd me with a pract,\n",
      "To the entreat the like as wor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate from the model\n",
    "texts = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decoding(m.generate(texts, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
